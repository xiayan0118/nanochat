{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9801d055",
   "metadata": {},
   "source": [
    "# Engine Class Playground\n",
    "\n",
    "This notebook explores the `Engine` class for efficient model inference with KV cache support.\n",
    "You can set breakpoints in VS Code's \"Run & Debug\" tool to inspect variables and understand the generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8c09e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import the Engine class and dependencies for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d3740be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# Add nanochat to path\n",
    "sys.path.insert(0, '/Users/yanxia/code/lab/nanochat')\n",
    "\n",
    "from nanochat.engine import Engine\n",
    "from nanochat.checkpoint_manager import load_model\n",
    "from nanochat.common import autodetect_device_type\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec55f64",
   "metadata": {},
   "source": [
    "## 2. Set Up Device and Load Model\n",
    "\n",
    "Initialize the device and load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50e1372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n",
      "Loading model...\n",
      "❌ Error: [Errno 2] No such file or directory: '/Users/yanxia/.cache/nanochat/base_checkpoints'\n",
      "\n",
      "No trained model found at ~/.cache/nanochat/base_checkpoints/\n",
      "\n",
      "To use this notebook, you have two options:\n",
      "1. Train a model first:\n",
      "   python -m scripts.base_train --depth=4 --max_seq_len=512 --num_iterations=100\n",
      "\n",
      "2. Or skip to the next section to mock the Engine setup for testing\n",
      "\n",
      "For now, we'll create a minimal setup to test the Engine class...\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device_type = autodetect_device_type()\n",
    "device = torch.device(\"cuda\" if device_type == \"cuda\" else \"cpu\")\n",
    "print(\"Loading model...\")\n",
    "try:\n",
    "    model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\")\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"  Model step: {meta['step']}\")\n",
    "    print(f\"  Model config: {meta['model_config']}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(f\"\\nNo trained model found at ~/.cache/nanochat/base_checkpoints/\")\n",
    "    print(f\"\\nTo use this notebook, you have two options:\")\n",
    "    print(f\"1. Train a model first:\")\n",
    "    print(f\"   python -m scripts.base_train --depth=4 --max_seq_len=512 --num_iterations=100\")\n",
    "    print(f\"\\n2. Or skip to the next section to mock the Engine setup for testing\")\n",
    "    print(f\"\\nFor now, we'll create a minimal setup to test the Engine class...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e580ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback: Create mock model/tokenizer for testing (if no checkpoint available)\n",
    "if 'model' not in locals():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Creating mock setup for Engine exploration...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    from nanochat.gpt import GPT, GPTConfig\n",
    "    from nanochat.tokenizer import get_tokenizer\n",
    "    \n",
    "    # Create a tiny model for testing\n",
    "    tokenizer = get_tokenizer()\n",
    "    model_config = GPTConfig(\n",
    "        sequence_len=512,\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        n_layer=2,  # Tiny: 2 layers instead of 20\n",
    "        n_head=2,\n",
    "        n_kv_head=2,\n",
    "        n_embd=64,\n",
    "    )\n",
    "    model = GPT(model_config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Mock model created successfully\")\n",
    "    print(f\"  Layers: 2 (reduced for testing)\")\n",
    "    print(f\"  Embedding dim: 64\")\n",
    "    print(f\"  Note: This model is untrained, outputs will be random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27988d",
   "metadata": {},
   "source": [
    "## 3. Create Engine Instance\n",
    "\n",
    "Initialize an Engine instance for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22b590fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Engine created successfully\n",
      "  Model: GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65536, 64)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (c_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (c_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=65536, bias=False)\n",
      ")\n",
      "  Tokenizer: RustBPETokenizer\n"
     ]
    }
   ],
   "source": [
    "# Create Engine instance\n",
    "engine = Engine(model, tokenizer)\n",
    "print(f\"✓ Engine created successfully\")\n",
    "print(f\"  Model: {engine.model}\")\n",
    "print(f\"  Tokenizer: {type(engine.tokenizer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6134694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is'\n",
      "Prompt tokens: [65527, 449, 3428, 281, 4092, 309]\n",
      "\n",
      "✓ Prompt ready. Now run the generation cells below to hit engine.py breakpoints!\n"
     ]
    }
   ],
   "source": [
    "# Prepare a prompt for generation\n",
    "bos_token_id = tokenizer.get_bos_token_id()\n",
    "prompt = \"The capital of France is\"\n",
    "prompt_tokens = tokenizer.encode(prompt, prepend=bos_token_id)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"\\n✓ Prompt ready. Now run the generation cells below to hit engine.py breakpoints!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c870e7",
   "metadata": {},
   "source": [
    "## 4. Test Engine Methods and Properties\n",
    "\n",
    "Test the `generate()` generator and `generate_batch()` batch method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "957e252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Test 1: Streaming Generation (generate)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooter escalateViews Duch-inchemiccart prorand Boom Corbusier motorized Filipino rise proceeding pol\n",
      "\n",
      "Generated token sequence:\n",
      "[37342, 39912, 57209, 36893, 12745, 2343, 57875, 347, 2789, 46970, 62399, 44208, 30103, 3547, 26747, 894]\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Streaming generation with generate()\n",
    "# ⚠️ Set breakpoints in engine.py BEFORE running this cell!\n",
    "print(\"=\" * 60)\n",
    "print(\"Test 1: Streaming Generation (generate)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generated_tokens = []\n",
    "for token_column, token_masks in engine.generate(  # <-- Breakpoints in engine.py will be hit here!\n",
    "    prompt_tokens, \n",
    "    num_samples=1, \n",
    "    max_tokens=16, \n",
    "    temperature=0.0,  # Greedy decoding\n",
    "    seed=42\n",
    "):\n",
    "    token = token_column[0]\n",
    "    generated_tokens.append(token)\n",
    "    text = tokenizer.decode([token])\n",
    "    print(text, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nGenerated token sequence:\")\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9fc647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test 2: Batch Generation (generate_batch)\n",
      "============================================================\n",
      "\n",
      "Sample 1: <|bos|>The capital of France is Miners dietsCow omission hog snapLarisl Adobearry conserve Sacrament\n",
      "  Token count: 18\n",
      "  Sampled masks: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Sample 2: <|bos|>The capital of France is pleasure-varrarilyactivity foundedalong chelation BottleTomorrowSnow sleeve harness\n",
      "  Token count: 18\n",
      "  Sampled masks: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Sample 3: <|bos|>The capital of France is Cylfacedhele termiteatham share seg�/journal southern Assessingemen\n",
      "  Token count: 18\n",
      "  Sampled masks: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Batch generation with generate_batch()\n",
    "# ⚠️ Set breakpoints in engine.py BEFORE running this cell!\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 2: Batch Generation (generate_batch)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results, masks = engine.generate_batch(  # <-- Breakpoints in engine.py will be hit here!\n",
    "    prompt_tokens,\n",
    "    num_samples=3,\n",
    "    max_tokens=12,\n",
    "    temperature=1.0,  # Sampling with temperature\n",
    "    top_k=50,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "for i, (tokens, mask) in enumerate(zip(results, masks)):\n",
    "    text = tokenizer.decode(tokens)\n",
    "    print(f\"\\nSample {i+1}: {text}\")\n",
    "    print(f\"  Token count: {len(tokens)}\")\n",
    "    print(f\"  Sampled masks: {mask}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991acb42",
   "metadata": {},
   "source": [
    "## 5. Inspect Engine State\n",
    "\n",
    "Examine the internal state and properties of the Engine instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Engine State Inspection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Inspect Engine attributes\n",
    "print(f\"\\nEngine attributes:\")\n",
    "print(f\"  engine.model type: {type(engine.model).__name__}\")\n",
    "print(f\"  engine.tokenizer type: {type(engine.tokenizer).__name__}\")\n",
    "\n",
    "# Inspect Model config\n",
    "model_config = engine.model.config\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  n_layer: {model_config.n_layer}\")\n",
    "print(f\"  n_embd: {model_config.n_embd}\")\n",
    "print(f\"  n_head: {model_config.n_head}\")\n",
    "print(f\"  n_kv_head: {model_config.n_kv_head}\")\n",
    "print(f\"  sequence_len: {model_config.sequence_len}\")\n",
    "\n",
    "# Inspect Tokenizer properties\n",
    "print(f\"\\nTokenizer properties:\")\n",
    "print(f\"  Vocab size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"  BOS token ID: {tokenizer.get_bos_token_id()}\")\n",
    "\n",
    "# Memory usage\n",
    "if device_type == \"cuda\":\n",
    "    print(f\"\\nCUDA Memory:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanochat (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
